{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, torch, warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = \"/content/drive/My Drive/Simple_Chest_XRay/\"\n",
    "data_path = \"/content/drive/My Drive/NIH_Chest_XRay/\"\n",
    "#data_path = \"/content/drive/My Drive/Harvard_Chest_XRay/\"\n",
    "\n",
    "sample_ratio = 1\n",
    "batch_size = 96\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, torch, warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.inception import InceptionOutputs\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def data_sampling(indices):\n",
    "  return torch.utils.data.sampler.SubsetRandomSampler(indices)\n",
    "\n",
    "#Data Preprocessing\n",
    "def data_preprocess(data_path, sample_ratio, batch_size):\n",
    "  # Create data transforms\n",
    "  data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "  # Get dataset from folder and apply data transforms\n",
    "  dataset = datasets.ImageFolder(root = \"{}data\".format(data_path), transform = data_transforms)\n",
    "    \n",
    "  # Get a sample of the data randomly\n",
    "  num_samples = int(len(dataset) * sample_ratio)\n",
    "  indices = np.random.choice(range(len(dataset)), num_samples, replace = False)\n",
    "\n",
    "  # Split the data into training, test, and validation sets\n",
    "  train_size = int(0.7 * num_samples)\n",
    "  test_size = int(0.2 * num_samples)\n",
    "  val_size = num_samples - train_size - test_size\n",
    "\n",
    "  train_indices = indices[ : train_size]\n",
    "  test_indices = indices[train_size : train_size + test_size]\n",
    "  val_indices = indices[train_size + test_size : ]\n",
    "\n",
    "  samples = [data_sampling(i) for i in [train_indices, test_indices, val_indices]]\n",
    "\n",
    "  # Create data loaders for training, test, and validation sets\n",
    "  train_loader = DataLoader(dataset, batch_size = batch_size, sampler = samples[0], num_workers = 4, pin_memory = True)\n",
    "  test_loader = DataLoader(dataset, batch_size = batch_size, sampler = samples[1], num_workers = 4, pin_memory = True)\n",
    "  val_loader = DataLoader(dataset, batch_size = batch_size, sampler = samples[2], num_workers = 4, pin_memory = True)\n",
    "\n",
    "  return dataset, train_loader, train_indices, test_loader, test_indices, val_loader, val_indices\n",
    "\n",
    "def evaluate_model(model, dataloader, data_size, dtype, criterion, data_path, model_name):\n",
    "  _loss, _pred, _true, _accuracy = 0.0, [], [], []\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in dataloader:\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      _loss += loss.item() * inputs.size(0)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      _pred.extend(predicted.cpu().numpy())\n",
    "      _true.extend(labels.cpu().numpy())\n",
    "\n",
    "  _loss /= len(data_size)\n",
    "  _accuracy = accuracy_score(_true, _pred)  \n",
    "  _recall = recall_score(_true, _pred, average='macro')\n",
    "  _precision = precision_score(_true, _pred, average='macro')\n",
    "  _fscore = f1_score(_true, _pred, average='macro')\n",
    "\n",
    "  print('{}: Accuracy: {:.4f} | Loss: {:.4f} | Recall: {:.4f} | Precision: {:.4f} | F-score: {:.4f}'.format(dtype, _accuracy, _loss, _recall, _precision, _fscore))\n",
    "  print(\"\")\n",
    "\n",
    "  if(dtype == \"TEST\"):\n",
    "    cm = confusion_matrix(_true, _pred)\n",
    "    plt.figure(figsize = (8, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = dataset.classes)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "  else:\n",
    "    return _accuracy, _loss\n",
    "  \"\"\"  \n",
    "    plt.imshow(cm, cmap = plt.cm.Blues)\n",
    "    plt.title(\"{}_{}SET_CONFUSION_MATRIX\".format(model_name, dtype))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(\"{}_{}SET_CONFUSION_MATRIX.png\".format(model_name, dtype))\n",
    "    \"\"\"\n",
    "\n",
    "def train_model(model, criterion, optimizer, model_name, num_epochs):\n",
    "  losses, accuracies, true, pred, v_accuracies, v_losses = [], [], [], [], [], []\n",
    "  for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = 0.0, 0.0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
    "      for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.logits if isinstance(outputs, InceptionOutputs) else outputs, dim = 1)\n",
    "        loss = criterion(outputs.logits if isinstance(outputs, InceptionOutputs) else outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_accuracy += torch.sum(preds == labels.data)\n",
    "        pred.extend(preds.cpu().numpy())\n",
    "        true.extend(labels.cpu().numpy())\n",
    "\n",
    "        pbar.set_postfix({'Accuracy': train_accuracy.item()/len(train_indices), 'Loss': train_loss/len(train_indices), 'Precision': precision_score(true, pred, average='macro'), 'Recall': recall_score(true, pred, average='macro'), 'F1 Score': f1_score(true, pred, average = 'macro')})\n",
    "        pbar.update()      \n",
    "    \n",
    "    val_accuracy, val_loss = evaluate_model(model, val_loader, val_indices, 'VALIDATION', criterion, data_path, \"ResNet18\")\n",
    "\n",
    "    v_accuracies.append(val_accuracy)\n",
    "    v_losses.append(val_loss)\n",
    "    losses.append(train_loss/len(train_indices))\n",
    "    accuracies.append(train_accuracy.item()/len(train_indices))\n",
    "  print(losses, v_losses)\n",
    "  save_metrics(losses, accuracies, model_name)\n",
    "  return losses, accuracies, v_accuracies, v_losses\n",
    "\n",
    "def plot_TSNE(train_loader, device, model):\n",
    "  #Obtain the TSNE Plot for the data\n",
    "  features = []\n",
    "  labels = []\n",
    "  for images, targets in train_loader:\n",
    "      images = images.to(device)\n",
    "      targets = targets.to(device)\n",
    "      with torch.no_grad():\n",
    "          output = model(images)\n",
    "          features.append(output.cpu().numpy())\n",
    "          labels.append(targets.cpu().numpy())\n",
    "\n",
    "  features = np.vstack(features)\n",
    "  labels = np.concatenate(labels)\n",
    "\n",
    "  tsne = TSNE(n_components=2, perplexity = 25, learning_rate = 600, n_iter = 900)\n",
    "  tsne_features = tsne.fit_transform(features)\n",
    "\n",
    "  tsne_df = pd.DataFrame(data=tsne_features, columns=['t-SNE 1', 't-SNE 2'])\n",
    "  tsne_df['label'] = labels\n",
    "\n",
    "  # Plot the t-SNE plot with seaborn\n",
    "  sns.scatterplot(data=tsne_df, x='t-SNE 1', y='t-SNE 2', hue='label', palette='tab10')\n",
    "  plt.title('t-SNE Plot')\n",
    "  plt.show()\n",
    "\n",
    "def plot_within_class_variance(dataset):\n",
    "  #Get the class labels and the number of classes\n",
    "  class_labels = dataset.classes\n",
    "  num_classes = len(class_labels)\n",
    "\n",
    "  #Get the number of images per class\n",
    "  num_images_per_class = []\n",
    "  for i in range(num_classes):\n",
    "      class_indices = np.where(np.array(dataset.targets) == i)[0]\n",
    "      num_images_per_class.append(len(class_indices))\n",
    "\n",
    "  #Compute the mean and variance of the images per class\n",
    "  mean_num_images = np.mean(num_images_per_class)\n",
    "  var_num_images = np.var(num_images_per_class)\n",
    "\n",
    "  #Plot the within-class variance\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.bar(class_labels, num_images_per_class)\n",
    "  ax.axhline(y=mean_num_images, linestyle='--', color='r', label='Mean')\n",
    "  ax.axhspan(mean_num_images - np.sqrt(var_num_images), mean_num_images + np.sqrt(var_num_images),\n",
    "            alpha=0.2, color='y', label='Variance')\n",
    "  ax.legend()\n",
    "  plt.xticks(rotation = 0)\n",
    "  plt.ylabel('Number of Images')\n",
    "  plt.xlabel('Classes')\n",
    "  plt.title('Within-Class Variance Plot')\n",
    "  plt.show()\n",
    "\n",
    "def plot_model_curves(losses, accuracies, v_accuracies, v_losses):\n",
    "  #Plotting the Loss and Accuracy Curves\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "  ax1.plot(losses, label = \"Training Loss\")\n",
    "  ax1.plot(v_losses, label = \"Validation Loss\")\n",
    "  ax1.set_xlabel('Epoch')\n",
    "  ax1.set_ylabel('Loss')\n",
    "  ax1.set_title('Training and Validation Loss Curve')\n",
    "  ax1.legend()\n",
    "\n",
    "  ax2.plot(accuracies, label = \"Training Accuracy\")\n",
    "  ax2.plot(v_accuracies, label = \"Validation Accuracy\")\n",
    "  ax2.set_xlabel('Epoch')\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_title('Training and Validation Accuracy Curve')\n",
    "  ax2.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, train_loader, train_indices, test_loader, test_indices, val_loader, val_indices = data_preprocess(data_path, sample_ratio, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(loss, accuracy, model):\n",
    "  np.save(\"{}{}_train_loss.npy\".format(data_path, model), loss)\n",
    "  np.save(\"{}{}_train_accuracy.npy\".format(data_path, model), accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet18 model and set Pretraining to False to train model from scratch\n",
    "model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained = False)\n",
    "model.fc = nn.Linear(512, len(dataset.classes))\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function as CrossEntropy and optimizer as Adam Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "losses, accuracies, v_accuracies, v_losses = train_model(model, criterion, optimizer, \"ResNet18\", num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"{}resnet18.pth\".format(data_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Loss and Accuracy Curves\n",
    "plot_model_curves(losses, accuracies, v_accuracies, v_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Model on Test Set\n",
    "evaluate_model(model, test_loader, test_indices, 'TEST', criterion, data_path, \"ResNet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain the TSNE Plot for the data\n",
    "plot_TSNE(train_loader, device, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels and the number of classes\n",
    "plot_within_class_variance(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_class_variance(dataset, model):\n",
    "    # Set the model to evaluation mode\n",
    "    model.load_state_dict(model['model_state_dict'])\n",
    "    model.eval()\n",
    "    # Get the feature vectors and labels for the dataset\n",
    "    features = []\n",
    "    labels = []\n",
    "    for images, targets in train_loader:\n",
    "        with torch.no_grad():\n",
    "          images = images.to(device)\n",
    "          targets = targets.to(device)\n",
    "          output = model(images)\n",
    "          features.append(output.cpu().numpy()[0])\n",
    "          labels.append(targets.cpu().numpy()[0])\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Calculate the within-class variance for each class\n",
    "    class_variances = []\n",
    "    for c in np.unique(labels):\n",
    "        class_features = features[labels == c]\n",
    "        class_mean = np.mean(class_features, axis=0)\n",
    "        class_variance = np.mean(np.sum((class_features - class_mean)**2, axis=1))\n",
    "        class_variances.append(class_variance)\n",
    "    \n",
    "    return class_variances\n",
    "\n",
    "# Define the dataset and model\n",
    "# Calculate the within-class variance\n",
    "class_variances = within_class_variance(dataset, model)\n",
    "\n",
    "# Plot the within-class variances for each class\n",
    "plt.bar(np.arange(len(class_variances)), class_variances)\n",
    "plt.xticks(np.arange(len(class_variances)), dataset.classes, rotation='vertical')\n",
    "plt.ylabel('Within-class variance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
